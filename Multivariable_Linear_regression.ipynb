{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxm1sxy2ZMUcwULjqwinlR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "여러개의 정보가 존재할 때 하나의 값을 추측"
      ],
      "metadata": {
        "id": "NYJ7ikq8jiDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Vh86jCiUtqfz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "LmjURXcCjtru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "z7K1b2FPifhm"
      },
      "outputs": [],
      "source": [
        "X_train = torch.FloatTensor([[73,80,75],\n",
        "                             [93,88,93],\n",
        "                             [89,91,90],\n",
        "                             [96,98,100],\n",
        "                             [73,66,70]])\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis Function: 인공신경망 구조를 나타냄\n",
        "\n",
        "H(x) = Wx + b (x:vector, W:matrix)\n",
        "\n",
        "H(x) = w1x1 + w2x2 + w3x3 + b (입력변수 3개 -> weight 3개)"
      ],
      "metadata": {
        "id": "3No-C44XkQy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypothesis = X1_train * w1 + X2_train + w2 + X3_train * w3 + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "k2Bi5vMVk2O_",
        "outputId": "a67d9097-9fa1-4c89-e603-bdaf57e508f2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X1_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-dc6793a8c15e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX1_train\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX2_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX3_train\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X1_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "matmul(): 한번에 계산 _ x의 길이가 바뀌어도 코드를 바꿀 필요가 없다."
      ],
      "metadata": {
        "id": "K8hiDRBklL0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypothesis = X_train.matmul(W) + b"
      ],
      "metadata": {
        "id": "oqUGC8a9lViq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function: MSE"
      ],
      "metadata": {
        "id": "jSBCpEKjt536"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost = torch.mean((hypothesis - y_train)**2)"
      ],
      "metadata": {
        "id": "vmhGogbit9El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "5Z7KZtjLuL0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim"
      ],
      "metadata": {
        "id": "vwa8VDCHudtI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.zeros((3,1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "dYqt_w8yuNa8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "id": "MOCbra3eunnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD([W,b], lr=1e-5)"
      ],
      "metadata": {
        "id": "7zJbfRXiuvpD"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "계산: Hypothesis, cost, gradient descent"
      ],
      "metadata": {
        "id": "h4QvEGXPu7Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  hypothesis = X_train.matmul(W) + b\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Epoch{:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
        "      epoch, nb_epochs, hypothesis.squeeze().detach(),\n",
        "      cost.item()\n",
        "  ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUoK88C7vMDy",
        "outputId": "0ac045ee-691f-43f7-f06e-64beb37d5b9b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
            "Epoch   1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
            "Epoch   2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
            "Epoch   3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
            "Epoch   4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936096\n",
            "Epoch   5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371063\n",
            "Epoch   6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) Cost: 29.758249\n",
            "Epoch   7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) Cost: 10.445267\n",
            "Epoch   8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391237\n",
            "Epoch   9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493121\n",
            "Epoch  10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
            "Epoch  11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) Cost: 1.710552\n",
            "Epoch  12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651416\n",
            "Epoch  13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632369\n",
            "Epoch  14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625924\n",
            "Epoch  15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623420\n",
            "Epoch  16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622152\n",
            "Epoch  17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621262\n",
            "Epoch  18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) Cost: 1.620501\n",
            "Epoch  19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) Cost: 1.619757\n",
            "Epoch  20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) Cost: 1.619046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cost 감소, H(x)는 y에 수렴, lr에 따라 발산할수도 있음"
      ],
      "metadata": {
        "id": "RiVG79ulwCOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Module: 파이토치에서 제공하는 모듈"
      ],
      "metadata": {
        "id": "HeVCPdjuwTDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n",
        "\n",
        "hypothesis = model(X_train)"
      ],
      "metadata": {
        "id": "EUeqo_yx1Dqm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Module을 상속해서 모델을 생성한다.\n",
        "\n",
        "nn.Linear(3,1)\n",
        "\n",
        "\n",
        "*   입력 차원: 3\n",
        "*   출력 차원: 1\n",
        "\n",
        "forward(): Hypothesis 계산\n",
        "\n",
        "PyTorch backward(): Gradient 계산\n",
        "\n"
      ],
      "metadata": {
        "id": "bm8Aqxih1EJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "F.mse_loss: torch.nn.functional에서 제공하는 loss function을 사용하여 cost를 계산한다."
      ],
      "metadata": {
        "id": "W6_LC_pP18ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "cost = F.mse_loss(prediction, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX1v25961yjH",
        "outputId": "15c1dd0d-dace-4f33-ef69-2b9aadc3ed62"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-81e4d6c260bc>:2: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  cost = F.mse_loss(prediction, y_train)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.optim 모델 정의하는 부분이 바뀌게 된다."
      ],
      "metadata": {
        "id": "8EBf77wc2a6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터"
      ],
      "metadata": {
        "id": "KI--8zgf2nAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.FloatTensor([[73,80,75],\n",
        "                             [93,88,93],\n",
        "                             [89,91,90],\n",
        "                             [96,98,100],\n",
        "                             [73,66,70]])\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
      ],
      "metadata": {
        "id": "5T1zxzX02Ihn"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 정의"
      ],
      "metadata": {
        "id": "VU9kTald2oLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n",
        "\n",
        "model = MultivariateLinearRegressionModel()"
      ],
      "metadata": {
        "id": "-2i7orNl2p-g"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer 설정"
      ],
      "metadata": {
        "id": "JaLuj7dN3Q4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD([W,b],lr=1e-5)"
      ],
      "metadata": {
        "id": "6icfkps03S26"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "계산: Hypothesis, cost, gradient descent"
      ],
      "metadata": {
        "id": "sAxDcjmr3l9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  Hypothesis = model(X_train)\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Epoch{:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
        "      epoch, nb_epochs, hypothesis.squeeze().detach(),\n",
        "      cost.item()\n",
        "  ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "EqMbqctD3lsH",
        "outputId": "71aa8d66-8697-4988-d34e-1edccd71152f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-c2f4d3fca44c>:4: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  cost = F.mse_loss(prediction, y_train)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-c2f4d3fca44c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real World의 Dataset은 적어도 수십만개이다.\n",
        "\n",
        "많은 데이터를 다 학습할 수 없어서 일부분의 데이터로만 학습한다.\n",
        "\n",
        "전체 데이터를 minibatch로 나누어서 학습한다."
      ],
      "metadata": {
        "id": "yI_CKUu84w3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch Dataset 만들기"
      ],
      "metadata": {
        "id": "rlN1VQYm5ovm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.utils.data.Dataset 상속"
      ],
      "metadata": {
        "id": "FOAeojQe5zyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "oet9rQNg3Yqa"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__len__(): dataset의 총 data수\n",
        "\n",
        "__getitem__(): index를 받았을 때, 반환하는 입출력 데이터"
      ],
      "metadata": {
        "id": "ymKJDOHS7FzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.X_data = ([[73,80,75],\n",
        "                    [93,88,93],\n",
        "                    [89,91,90],\n",
        "                    [96,98,100],\n",
        "                    [73,66,70]])\n",
        "    self.y_data = ([[152],[185],[180],[196],[142]])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    X = torch.FloatTensor(self.X_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "dataset = CustomDataset()"
      ],
      "metadata": {
        "id": "iiRLbB-l5RLi"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch DataLoader"
      ],
      "metadata": {
        "id": "myE59vDi8pmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "qFLwrN_O8gzA"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size = 2\n",
        "- 각 미니배치의 크기이다. 통상적으로 2의 제곱수로 설정\n",
        "\n",
        "shuffle=True\n",
        "- Epoch마다 데이터셋을 섞는다. 데이터가 학습되는 순서를 바꾼다.\n",
        "\n"
      ],
      "metadata": {
        "id": "PWYSj_a-8-K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    X_train, y_train = samples\n",
        "\n",
        "    prediction = model(X_train)\n",
        "\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    print('Epoch{:4d}/{} Batch {} Cost: {:.6f}'.format(\n",
        "      epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "      cost.item()\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6tR5VjF88ro",
        "outputId": "cd715677-7c51-42cc-c647-7e4cc773177c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0/20 Batch 1 Cost: 3.000000\n",
            "Epoch   0/20 Batch 2 Cost: 3.000000\n",
            "Epoch   0/20 Batch 3 Cost: 3.000000\n",
            "Epoch   1/20 Batch 1 Cost: 3.000000\n",
            "Epoch   1/20 Batch 2 Cost: 3.000000\n",
            "Epoch   1/20 Batch 3 Cost: 3.000000\n",
            "Epoch   2/20 Batch 1 Cost: 3.000000\n",
            "Epoch   2/20 Batch 2 Cost: 3.000000\n",
            "Epoch   2/20 Batch 3 Cost: 3.000000\n",
            "Epoch   3/20 Batch 1 Cost: 3.000000\n",
            "Epoch   3/20 Batch 2 Cost: 3.000000\n",
            "Epoch   3/20 Batch 3 Cost: 3.000000\n",
            "Epoch   4/20 Batch 1 Cost: 3.000000\n",
            "Epoch   4/20 Batch 2 Cost: 3.000000\n",
            "Epoch   4/20 Batch 3 Cost: 3.000000\n",
            "Epoch   5/20 Batch 1 Cost: 3.000000\n",
            "Epoch   5/20 Batch 2 Cost: 3.000000\n",
            "Epoch   5/20 Batch 3 Cost: 3.000000\n",
            "Epoch   6/20 Batch 1 Cost: 3.000000\n",
            "Epoch   6/20 Batch 2 Cost: 3.000000\n",
            "Epoch   6/20 Batch 3 Cost: 3.000000\n",
            "Epoch   7/20 Batch 1 Cost: 3.000000\n",
            "Epoch   7/20 Batch 2 Cost: 3.000000\n",
            "Epoch   7/20 Batch 3 Cost: 3.000000\n",
            "Epoch   8/20 Batch 1 Cost: 3.000000\n",
            "Epoch   8/20 Batch 2 Cost: 3.000000\n",
            "Epoch   8/20 Batch 3 Cost: 3.000000\n",
            "Epoch   9/20 Batch 1 Cost: 3.000000\n",
            "Epoch   9/20 Batch 2 Cost: 3.000000\n",
            "Epoch   9/20 Batch 3 Cost: 3.000000\n",
            "Epoch  10/20 Batch 1 Cost: 3.000000\n",
            "Epoch  10/20 Batch 2 Cost: 3.000000\n",
            "Epoch  10/20 Batch 3 Cost: 3.000000\n",
            "Epoch  11/20 Batch 1 Cost: 3.000000\n",
            "Epoch  11/20 Batch 2 Cost: 3.000000\n",
            "Epoch  11/20 Batch 3 Cost: 3.000000\n",
            "Epoch  12/20 Batch 1 Cost: 3.000000\n",
            "Epoch  12/20 Batch 2 Cost: 3.000000\n",
            "Epoch  12/20 Batch 3 Cost: 3.000000\n",
            "Epoch  13/20 Batch 1 Cost: 3.000000\n",
            "Epoch  13/20 Batch 2 Cost: 3.000000\n",
            "Epoch  13/20 Batch 3 Cost: 3.000000\n",
            "Epoch  14/20 Batch 1 Cost: 3.000000\n",
            "Epoch  14/20 Batch 2 Cost: 3.000000\n",
            "Epoch  14/20 Batch 3 Cost: 3.000000\n",
            "Epoch  15/20 Batch 1 Cost: 3.000000\n",
            "Epoch  15/20 Batch 2 Cost: 3.000000\n",
            "Epoch  15/20 Batch 3 Cost: 3.000000\n",
            "Epoch  16/20 Batch 1 Cost: 3.000000\n",
            "Epoch  16/20 Batch 2 Cost: 3.000000\n",
            "Epoch  16/20 Batch 3 Cost: 3.000000\n",
            "Epoch  17/20 Batch 1 Cost: 3.000000\n",
            "Epoch  17/20 Batch 2 Cost: 3.000000\n",
            "Epoch  17/20 Batch 3 Cost: 3.000000\n",
            "Epoch  18/20 Batch 1 Cost: 3.000000\n",
            "Epoch  18/20 Batch 2 Cost: 3.000000\n",
            "Epoch  18/20 Batch 3 Cost: 3.000000\n",
            "Epoch  19/20 Batch 1 Cost: 3.000000\n",
            "Epoch  19/20 Batch 2 Cost: 3.000000\n",
            "Epoch  19/20 Batch 3 Cost: 3.000000\n",
            "Epoch  20/20 Batch 1 Cost: 3.000000\n",
            "Epoch  20/20 Batch 2 Cost: 3.000000\n",
            "Epoch  20/20 Batch 3 Cost: 3.000000\n"
          ]
        }
      ]
    }
  ]
}